
params {
  config_profile_description = 'Biowulf nf-core config'
  config_profile_contact = 'staff@hpc.nih.gov'
  config_profile_url = 'https://hpc.nih.gov/apps/nextflow.html'
  max_memory = '224 GB'
  max_cpus = 32
  max_time = '72 h'

  igenomes_base = '/fdb/igenomes/'
}

// container_mounts =  '-B/gs10,/gs11,/gs12,/gs6,/gs8,/gs9,/vf,/spin1,/data,/fdb,/gpfs,/lscratch'

// use a local executor for short jobs. For this the
// settings below may have to be adapted to the allocation for
// the main nextflow job
executor {
    $local {
        queueSize = 100
        memory = '72 G'
        cpus = '24'

    }
    $slurm {
        queue = 'norm'
        queueSize = 200
        pollInterval = '2 min'
        queueStatInterval = '5 min'
        submitRateLimit = '6/1min'
        retry.maxAttempts = 1
    }
}

singularity {
    enabled = true
    autoMounts = true
    cacheDir = "/data/$USER/singularity"
    envWhitelist='https_proxy,http_proxy,ftp_proxy,DISPLAY,SLURM_JOBID,SINGULARITY_BINDPATH'
}

env {
    SINGULARITY_CACHEDIR="/data/$USER/singularity"
    PYTHONNOUSERSITE = 1
}

profiles {
    biowulflocal {
        process.executor = 'local'
        process.cache = 'lenient'

    }

    biowulf {
        process {
            executor = 'slurm'
            errorStrategy = { task.exitStatus in 137..140 ? 'retry' : 'finish' }
            maxRetries = 1

            clusterOptions = ' --gres=lscratch:200 '

            scratch = '/lscratch/$SLURM_JOBID'
            // with the default stageIn and stageOut settings using scratch can
            // result in humungous work folders
            // see https://github.com/nextflow-io/nextflow/issues/961 and
            //     https://www.nextflow.io/docs/latest/process.html?highlight=stageinmode
            stageInMode = 'symlink'
            stageOutMode = 'rsync'

            // for running pipeline on group sharing data directory, this can avoid inconsistent files timestamps
            cache = 'lenient'

        // example for setting different parameters for jobs with a 'gpu' label
        // withLabel:gpu {
        //    queue = 'gpu'
        //    time = '36h'
        //    clusterOptions = " --gres=lscratch:400,gpu:v100x:1 "
        //    containerOptions = " --nv "
        // }

        // example for setting different parameters for a process name
        //  withName: 'FASTP|MULTIQC' {
        //  cpus = 6
        //  queue = 'quick'
        //  memory = '6 GB'
        //  time = '4h'
        // }

        // example for setting different parameters for jobs with a resource label
        //  withLabel:process_low {
        //  cpus = 2
        //  memory = '12 GB'
        //  time = '4h'
        // }
        // withLabel:process_medium {
        //  cpus = 6
        //  memory = '36 GB'
        //  time = '12h'
        // }
        // withLabel:process_high {
        //  cpus = 12
        //  memory = '72 GB'
        //  time = '16 h'
        // }
     }
        timeline.enabled = true
        report.enabled = true
    }
}


